{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation_only_querry_ex.txt\n",
    "folder = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\build-TemporalSummarization-Desktop_Qt_5_5_1_MinGW_32bit-Release\\\\\"\n",
    "evaluation_1_filename = folder + \"Evaluation_notemp_noimp.txt\"\n",
    "evaluation_2_filename = folder + \"Evaluation_allin.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_eva_from_tag(file_discr, tag):\n",
    "    all_eva_vec = []\n",
    "    for line in file_discr:\n",
    "        find_res = line.find(tag)\n",
    "        if find_res != -1:\n",
    "            all_eva_vec.append(float(line[find_res + len(tag):]))\n",
    "            \n",
    "    file_discr.seek(0)\n",
    "    return [all_eva_vec[15:], all_eva_vec[:15]]\n",
    "    \n",
    "eva_1 = open(evaluation_1_filename)\n",
    "eva_2 = open(evaluation_2_filename)\n",
    "recall_tag = \"\\tRecall: \"\n",
    "precision_tag = \"\\tPrecision: \"\n",
    "f_tag = \"\\tF-m: \"\n",
    "p_sent_tag = \"\\tNuggetsRecall: \"\n",
    "\n",
    "eva_1_recalls = get_eva_from_tag(eva_1, recall_tag)\n",
    "eva_1_precisions = get_eva_from_tag(eva_1, precision_tag)\n",
    "eva_1_F_meashures = get_eva_from_tag(eva_1, f_tag)\n",
    "eva_1_p_sents = get_eva_from_tag(eva_1, p_sent_tag)\n",
    "\n",
    "\n",
    "eva_2_recalls = get_eva_from_tag(eva_2, recall_tag)\n",
    "eva_2_precisions = get_eva_from_tag(eva_2, precision_tag)\n",
    "eva_2_F_meashures = get_eva_from_tag(eva_2, f_tag)\n",
    "eva_2_p_sents = get_eva_from_tag(eva_2, p_sent_tag)\n",
    "\n",
    "R1_eva_1 = eva_1_recalls[0]\n",
    "R2_eva_1 = eva_1_recalls[1]\n",
    "P1_eva_1 = eva_1_precisions[0]\n",
    "P2_eva_1 = eva_1_precisions[1]\n",
    "F1_eva_1 = eva_1_F_meashures[0]\n",
    "F2_eva_1 = eva_1_F_meashures[1]\n",
    "Psent1_eva_1 = eva_1_p_sents[0]\n",
    "Psent2_eva_1 = eva_1_p_sents[1]\n",
    "\n",
    "R1_eva_2 = eva_2_recalls[0]\n",
    "R2_eva_2 = eva_2_recalls[1]\n",
    "P1_eva_2 = eva_2_precisions[0]\n",
    "P2_eva_2 = eva_2_precisions[1]\n",
    "F1_eva_2 = eva_2_F_meashures[0]\n",
    "F2_eva_2 = eva_2_F_meashures[1]\n",
    "Psent1_eva_2 = eva_2_p_sents[0]\n",
    "Psent2_eva_2 = eva_2_p_sents[1]\n",
    "    \n",
    "metrics_eva_1 = [R1_eva_1, R2_eva_1, P1_eva_1, P2_eva_1, F1_eva_1, F2_eva_1, Psent1_eva_1, Psent2_eva_1]\n",
    "metrics_eva_2 = [R1_eva_2, R2_eva_2, P1_eva_2, P2_eva_2, F1_eva_2, F2_eva_2, Psent1_eva_2, Psent2_eva_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#simulate data\n",
    "import numpy\n",
    "\n",
    "def gen_not_real_data(metric, size):\n",
    "    mean = numpy.mean(metric)\n",
    "    std = numpy.std(metric)\n",
    "    \n",
    "    return numpy.random.normal(mean, std, size).tolist()\n",
    "    \n",
    "\n",
    "metrics_size = len(metrics_eva_1)\n",
    "#for i in range(0, metrics_size):\n",
    "    #metrics_eva_1[i] += gen_not_real_data(metrics_eva_1[i], 15)\n",
    "    #metrics_eva_2[i] += gen_not_real_data(metrics_eva_2[i], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625 | 0.125 diff = 0.0625\n",
      "1.0 | 1.0 diff = 0.0\n",
      "0.1 | 0.1 diff = 0.0\n",
      "0.0 | 0.0 diff = 0.0\n",
      "0.588235 | 0.529412 diff = -0.05882299999999996\n",
      "0.5 | 0.5 diff = 0.0\n",
      "0.5625 | 0.53125 diff = -0.03125\n",
      "0.0 | 0.0625 diff = 0.0625\n",
      "0.125 | 0.1875 diff = 0.0625\n",
      "0.142857 | 0.142857 diff = 0.0\n",
      "0.0 | 0.0 diff = 0.0\n",
      "0.230769 | 0.153846 diff = -0.07692299999999999\n",
      "0.133333 | 0.133333 diff = 0.0\n",
      "0.466667 | 0.4 diff = -0.06666699999999998\n",
      "0.0 | 0.0714286 diff = 0.0714286\n",
      "0.02526560000000007\n",
      "0.001684373333333338\n"
     ]
    }
   ],
   "source": [
    "metrics_size = len(metrics_eva_1)\n",
    "metr_1 = metrics_eva_1[6]\n",
    "metr_2 = metrics_eva_2[6]\n",
    "\n",
    "size = len(metr_1)\n",
    "diff_sum = 0\n",
    "for i in range(0, size):\n",
    "    print(str(metr_1[i]) + \" | \" + str(metr_2[i]) + \" diff = \" + str(metr_2[i] - metr_1[i]))\n",
    "    diff_sum += metr_2[i] - metr_1[i]\n",
    "    \n",
    "print(diff_sum)\n",
    "print(diff_sum / size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg = 8 15\n",
      "0.94\n",
      "neg = 6 15\n",
      "0.7622\n",
      "neg = 7 15\n",
      "0.5514\n",
      "neg = 7 15\n",
      "0.7652\n",
      "neg = 7 15\n",
      "0.6482\n",
      "neg = 7 15\n",
      "0.9668\n",
      "neg = 4 15\n",
      "0.9274\n",
      "neg = 0 15\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def abs_diff(list1, list2, size):\n",
    "    return abs((sum(list1) / size) - (sum(list2) / size))\n",
    "\n",
    "def rand_labeling(metric_1, metric_2):\n",
    "    labeled_1 = []\n",
    "    labeled_2 = []\n",
    "    \n",
    "    for i in range(0, len(metric_1)):\n",
    "        if random.random() > 0.5:\n",
    "            labeled_1.append(metric_1[i])\n",
    "            labeled_2.append(metric_2[i])\n",
    "        else:\n",
    "            labeled_2.append(metric_1[i])\n",
    "            labeled_1.append(metric_2[i])\n",
    "    \n",
    "    return [labeled_1, labeled_2]\n",
    "\n",
    "def calc_p_value(metric_eva_1, metric_eva_2, num_of_tests):\n",
    "    if len(metric_eva_1) != len(metric_eva_2):\n",
    "        return -1\n",
    "    size = len(metric_eva_1)\n",
    "    if size < 1 or num_of_tests < 1:\n",
    "        return -1\n",
    "    \n",
    "    diff = abs_diff(metric_eva_1, metric_eva_2, size)\n",
    "    concat_metrics = metric_eva_1 + metric_eva_2\n",
    "    count = 0;\n",
    "    for i in range(0, num_of_tests):\n",
    "        #random.shuffle(concat_metrics)\n",
    "        labels = rand_labeling(metric_eva_1, metric_eva_2)\n",
    "        label_1 = labels[0]\n",
    "        label_2 = labels[1]\n",
    "        cur_diff = abs_diff(label_1, label_2, size)\n",
    "        if cur_diff > diff:\n",
    "            count += 1\n",
    "    \n",
    "    return count / num_of_tests\n",
    "\n",
    "metrics_size = len(metrics_eva_1)\n",
    "for i in range(0, metrics_size):\n",
    "    neg = 0\n",
    "    for j in range(0, len(metrics_eva_1[i])):\n",
    "        if metrics_eva_1[i][j] > metrics_eva_2[i][j]:\n",
    "            neg += 1\n",
    "    print(\"neg = \" + str(neg) + \" \"+  str(len(metrics_eva_1[i])))\n",
    "    print(calc_p_value(metrics_eva_1[i], metrics_eva_2[i], 5000))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
