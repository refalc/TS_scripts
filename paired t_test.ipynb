{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluation_only_querry_ex.txt\n",
    "folder = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\saved\\\\\"\n",
    "evaluation_1_filename = folder + \"evaluation_notemp_noimp_07_12_17.txt\"\n",
    "evaluation_2_filename = folder + \"evaluation_allin_07_12_17.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_evaluation_by_tag(text, tag):\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    stories = re.findall(r\"<story(.*?)>(.*?)</story>\", text)\n",
    "    #print(stories)\n",
    "    values = []\n",
    "    for story in stories:\n",
    "        pattern = r\"<\" + re.escape(tag) + r\">(.*?)</\" + re.escape(tag) + r\">\"\n",
    "        tag_data = re.search(pattern, story[1])\n",
    "        values.append(float(tag_data.group(1)))\n",
    "            \n",
    "    return values\n",
    "        \n",
    "metrics_names = [\"R1\", \"P1\", \"F1\", \"Psent1\", \"R2\", \"P2\", \"F2\"]\n",
    "\n",
    "eva_1 = open(evaluation_1_filename).read()\n",
    "eva_2 = open(evaluation_2_filename).read()\n",
    "\n",
    "metrics_eva_1 = []\n",
    "metrics_eva_2 = []\n",
    "for metric_name in metrics_names:\n",
    "    metrics_eva_1.append(get_evaluation_by_tag(eva_1, metric_name))\n",
    "    metrics_eva_2.append(get_evaluation_by_tag(eva_2, metric_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#simulate data\n",
    "import numpy\n",
    "\n",
    "def gen_not_real_data(metric, size):\n",
    "    mean = numpy.mean(metric)\n",
    "    std = numpy.std(metric)\n",
    "    \n",
    "    return numpy.random.normal(mean, std, size).tolist()\n",
    "    \n",
    "\n",
    "metrics_size = len(metrics_eva_1)\n",
    "#for i in range(0, metrics_size):\n",
    "    #metrics_eva_1[i] += gen_not_real_data(metrics_eva_1[i], 15)\n",
    "    #metrics_eva_2[i] += gen_not_real_data(metrics_eva_2[i], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg = 8 15\n",
      "0.9354\n",
      "neg = 7 15\n",
      "0.5488\n",
      "neg = 7 15\n",
      "0.6638\n",
      "neg = 6 15\n",
      "0.2328\n",
      "neg = 5 15\n",
      "0.6424\n",
      "neg = 7 15\n",
      "0.8134\n",
      "neg = 7 15\n",
      "0.9964\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def abs_diff(list1, list2, size):\n",
    "    return abs((sum(list1) / size) - (sum(list2) / size))\n",
    "\n",
    "def rand_labeling(metric_1, metric_2):\n",
    "    labeled_1 = []\n",
    "    labeled_2 = []\n",
    "    \n",
    "    for i in range(0, len(metric_1)):\n",
    "        if random.random() > 0.5:\n",
    "            labeled_1.append(metric_1[i])\n",
    "            labeled_2.append(metric_2[i])\n",
    "        else:\n",
    "            labeled_2.append(metric_1[i])\n",
    "            labeled_1.append(metric_2[i])\n",
    "    \n",
    "    return [labeled_1, labeled_2]\n",
    "\n",
    "def calc_p_value(metric_eva_1, metric_eva_2, num_of_tests):\n",
    "    if len(metric_eva_1) != len(metric_eva_2):\n",
    "        return -1\n",
    "    size = len(metric_eva_1)\n",
    "    if size < 1 or num_of_tests < 1:\n",
    "        return -1\n",
    "    \n",
    "    diff = abs_diff(metric_eva_1, metric_eva_2, size)\n",
    "    concat_metrics = metric_eva_1 + metric_eva_2\n",
    "    count = 0;\n",
    "    for i in range(0, num_of_tests):\n",
    "        #random.shuffle(concat_metrics)\n",
    "        labels = rand_labeling(metric_eva_1, metric_eva_2)\n",
    "        label_1 = labels[0]\n",
    "        label_2 = labels[1]\n",
    "        cur_diff = abs_diff(label_1, label_2, size)\n",
    "        if cur_diff > diff:\n",
    "            count += 1\n",
    "    \n",
    "    return count / num_of_tests\n",
    "\n",
    "metrics_size = len(metrics_eva_1)\n",
    "for i in range(0, metrics_size):\n",
    "    neg = 0\n",
    "    for j in range(0, len(metrics_eva_1[i])):\n",
    "        if metrics_eva_1[i][j] > metrics_eva_2[i][j]:\n",
    "            neg += 1\n",
    "    print(\"neg = \" + str(neg) + \" \"+  str(len(metrics_eva_1[i])))\n",
    "    print(calc_p_value(metrics_eva_1[i], metrics_eva_2[i], 5000))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
