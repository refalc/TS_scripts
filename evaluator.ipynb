{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "import codecs\n",
    "\n",
    "def read_stop_words(file_name):\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    stop_words = file.read().lower().split()\n",
    "\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "\n",
    "### ANSWER PARSER\n",
    "def GetNormalForm(text) :\n",
    "    if type(text) != type(\"str\") :\n",
    "        return []\n",
    "    morph_data = mystem.analyze(text)\n",
    "    normal_forms = []    \n",
    "    for data in morph_data :\n",
    "        if len(data) == 0 :\n",
    "            continue\n",
    "\n",
    "        if len(data[\"analysis\"]) == 0 :\n",
    "            continue\n",
    "    \n",
    "        normal_forms.append(data[\"analysis\"][0][\"lex\"])\n",
    "        \n",
    "    return normal_forms\n",
    "\n",
    "def GetNormalFormPymorphy2(text):\n",
    "    words = text.split()\n",
    "    normal_forms = []\n",
    "    for word in words:\n",
    "        morph_data = morph.parse(word)\n",
    "        if len(morph_data) > 0:\n",
    "            normal_forms.append(morph_data[0].normal_form)\n",
    "            \n",
    "    return normal_forms\n",
    "            \n",
    "def clean_text_data(text):\n",
    "    punct_set = string.punctuation\n",
    "    punct_set += '»'\n",
    "    punct_set += '«'\n",
    "    punct_set += '“'\n",
    "    punct_set += '„'\n",
    "    translator = str.maketrans('', '', punct_set)\n",
    "    \n",
    "    text = text.translate(translator).lower()\n",
    "    splited_text = text.split()\n",
    "    cleared_list = []\n",
    "    for word in splited_text:\n",
    "        if word not in stop_words:\n",
    "            cleared_list.append(word)\n",
    "    \n",
    "    text = ' '.join(word for word in cleared_list)\n",
    "    \n",
    "    return text\n",
    "def answer_parser(file_name):\n",
    "    answer_data = dict()\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    text = file.read()\n",
    "    \n",
    "    #del newline\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    # del metadata\n",
    "    text = re.sub(r\"<metadata(.*?)>\", \"\", text)\n",
    "    \n",
    "    #del querry data\n",
    "    text = re.sub(r\"<querries>(.*?)</querries>\", \"\", text)\n",
    "    stories = re.findall(r\"(<story.*?)</story>\", text)\n",
    "    for story in stories:\n",
    "        story_id = \"\"\n",
    "        if re.search(r\"init_doc_id=(\\d*)\", story) :\n",
    "            story_id = re.search(r\"init_doc_id=(\\d*)\", story).group(1)\n",
    "        else:\n",
    "            story_id = re.search(r\"story id=(\\d*)\", story).group(1)\n",
    "            \n",
    "        sentences = re.findall(r\"(<sentence.*?)</sentence>\", story)\n",
    "        answer_data[story_id] = []\n",
    "        for sentence in sentences:\n",
    "            sent_data = re.search(r\"<sentence id=(\\d*)(.*?)>(.*)\", sentence)\n",
    "            \n",
    "            sent_text = sent_data.group(3)\n",
    "            sent_text = clean_text_data(sent_text)\n",
    "            answer_data[story_id].append(sent_text)\n",
    "            \n",
    "    return answer_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_mapping(file_name):\n",
    "    mapping = dict()\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    text = file.read()\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    \n",
    "    pairs = re.findall(r\"<pair>(.*?)</pair>\", text)\n",
    "    for pair in pairs:\n",
    "        story_id = re.search(r\"<id>(\\d*)</id>\", pair).group(1)\n",
    "        queries = re.findall(r\"<doc_id>(.*?)</doc_id>\", pair)\n",
    "        for query in queries:\n",
    "            mapping[query] = story_id\n",
    "            \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ngramms(input_strs, N):\n",
    "    ngramms = []\n",
    "    for i in range(0, len(input_strs) - N + 1):\n",
    "        ngramm = input_strs[i]\n",
    "        for j in range(i + 1, i + N):\n",
    "            ngramm += \" \" + input_strs[j]\n",
    "        ngramms.append(ngramm)\n",
    "\n",
    "    return set(ngramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "metrics_names = [\"R1\", \"P1\", \"F1\", \"Psent1\", \"R2\", \"P2\", \"F2\"]\n",
    "\n",
    "def compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent):\n",
    "    all_retr = set()\n",
    "    all_rel = set()\n",
    "    for sentence_data in retr_ngrams_by_sent:\n",
    "        all_retr.update(sentence_data)\n",
    "        \n",
    "    for sentence_data in rel_ngrams_by_sent:\n",
    "        all_rel.update(sentence_data)\n",
    "        \n",
    "    RPF = [0, 0, 0]\n",
    "    if len(all_retr) > 0 and len(all_rel) > 0:\n",
    "        P = len(all_rel.intersection(all_retr)) / len(all_retr)\n",
    "        R = len(all_rel.intersection(all_retr)) / len(all_rel)\n",
    "        F = 2 * P * R / (P + R)\n",
    "        RPF[0] = R\n",
    "        RPF[1] = P\n",
    "        RPF[2] = F\n",
    "        \n",
    "    return RPF\n",
    "        \n",
    "def create_embeding_for_ngramm(ngramm, model):\n",
    "    vec = numpy.zeros(100)\n",
    "    splited = ngramm.split()\n",
    "    \n",
    "    count = 0\n",
    "    for word in splited:\n",
    "        n_form = GetNormalFormPymorphy2(word)[0]\n",
    "        if n_form in model.wv.vocab:\n",
    "            vec += model[n_form]\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def create_embeddings(ngramms, model):\n",
    "    embeddings = []\n",
    "    for ngramm in ngramms:\n",
    "        embeddings.append(create_embeding_for_ngramm(ngramm, model))\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def sentences_intersect_by_embedd(sent_retr, sent_rel):\n",
    "    intersect_count = 0\n",
    "    red_line = 0.5\n",
    "    for ngramm_emb_rel in sent_rel:\n",
    "        for ngramm_emb_retr in sent_retr:\n",
    "            if cosine_similarity([ngramm_emb_rel], [ngramm_emb_retr])[0][0] > red_line:\n",
    "                intersect_count += 1\n",
    "                break\n",
    "    \n",
    "    return intersect_count\n",
    "\n",
    "def compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model):\n",
    "    global create_embedings_count \n",
    "    global all_p_sent_time \n",
    "    start = time.clock()\n",
    "    red_line = 0.7\n",
    "    success_hits = 0\n",
    "    \n",
    "    vecs_for_set_sentence_rel = []\n",
    "    for rel_sentence_ngramms in rel_ngrams_by_sent:\n",
    "        vecs_for_set_sentence_rel.append(create_embeddings(rel_sentence_ngramms, model))\n",
    "       \n",
    "    vecs_for_set_sentence_retr = []\n",
    "    for retr_sentence_ngramms in retr_ngrams_by_sent:\n",
    "        vecs_for_set_sentence_retr.append(create_embeddings(retr_sentence_ngramms, model))\n",
    "        \n",
    "    end_creating_embedding = time.clock()\n",
    "    create_embedings_count += end_creating_embedding - start\n",
    "    for rel_sentence_embedds in vecs_for_set_sentence_rel:\n",
    "        for retr_sentence_embedds in vecs_for_set_sentence_retr:\n",
    "            intersect_count = sentences_intersect_by_embedd(retr_sentence_embedds, rel_sentence_embedds)\n",
    "            cur_score = intersect_count / len(rel_sentence_embedds) \n",
    "            if cur_score > red_line:\n",
    "                success_hits += 1\n",
    "                break\n",
    "    \n",
    "    end = time.clock()\n",
    "    all_p_sent_time += end - start\n",
    "    return success_hits / len(rel_ngrams_by_sent)\n",
    "    \n",
    "def compute_all_metrics(metrics, answers, golds, mapping, model):\n",
    "    count = 0\n",
    "    all_size = len(answers)\n",
    "    for story in answers:\n",
    "        retrieved_sentences = answers[story]\n",
    "        relevant_sentences = golds[mapping[story]]\n",
    "        retr_ngrams_by_sent = []\n",
    "        rel_ngrams_by_sent = []\n",
    "        for sentence in retrieved_sentences:\n",
    "            retr_ngrams_sent = create_ngramms(sentence.split(), 1)\n",
    "            retr_ngrams_by_sent.append(set(retr_ngrams_sent))\n",
    "        for sentence in relevant_sentences:\n",
    "            rel_ngrams_sent = create_ngramms(sentence.split(), 1)\n",
    "            rel_ngrams_by_sent.append(set(rel_ngrams_sent))\n",
    "\n",
    "        RPF = compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent)\n",
    "        metrics[\"R1\"][story] = RPF[0]\n",
    "        metrics[\"P1\"][story] = RPF[1]\n",
    "        metrics[\"F1\"][story] = RPF[2]\n",
    "        #metrics[\"Psent1\"][story] = compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model)\n",
    "        \n",
    "        retr_ngrams_by_sent = []\n",
    "        rel_ngrams_by_sent = []\n",
    "        for sentence in retrieved_sentences:\n",
    "            retr_ngrams_sent = create_ngramms(sentence.split(), 2)\n",
    "            retr_ngrams_by_sent.append(set(retr_ngrams_sent))\n",
    "        for sentence in relevant_sentences:\n",
    "            rel_ngrams_sent = create_ngramms(sentence.split(), 2)\n",
    "            rel_ngrams_by_sent.append(set(rel_ngrams_sent))\n",
    "    \n",
    "        RPF = compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent)\n",
    "        metrics[\"R2\"][story] = RPF[0]\n",
    "        metrics[\"P2\"][story] = RPF[1]\n",
    "        metrics[\"F2\"][story] = RPF[2]\n",
    "        #metrics[\"Psent2\"][story] = compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model)\n",
    "        if count % 1 == 0:\n",
    "            print(str(100 * count / all_size) + \"%\", end=\"\\r\")\n",
    "        count += 1\n",
    "    \n",
    "    return metrics\n",
    "        \n",
    "        \n",
    "def compute_mean_for_metric(metric):\n",
    "    mean = 0\n",
    "    for story in metric:\n",
    "         mean += metric[story]\n",
    "    if len(metric) > 0:\n",
    "        mean /= len(metric)\n",
    "    return mean\n",
    "\n",
    "def create_tag(tag_name, params, data):\n",
    "    tag = \"<\" + tag_name\n",
    "    for param in params:\n",
    "        tag += \" \" + param + \"=\" + str(params[param])\n",
    "    tag += \">\" + str(data) + \"</\" + tag_name + \">\\n\"\n",
    "    return tag\n",
    "\n",
    "def save_evaluation_in_file(configuration, results, file_name):    \n",
    "    config_tag = create_tag(\"configuration\", {}, \"\")\n",
    "    \n",
    "    #create set of stories\n",
    "    stories = set()\n",
    "    for result in results:\n",
    "        for story in results[result]:\n",
    "            if story not in stories:\n",
    "                stories.add(story)\n",
    "            \n",
    "    result_by_story = {}\n",
    "\n",
    "    for story_name in stories:\n",
    "        for result in results:\n",
    "            if story_name in results[result]:\n",
    "                if story_name not in result_by_story:\n",
    "                    result_by_story[story_name] = {}\n",
    "                    \n",
    "                result_by_story[story_name][result] = results[result][story_name] \n",
    "    \n",
    "    newline = \"\\n\"\n",
    "    #create means_tag\n",
    "    mean_metric_data = \"\"\n",
    "    for metric in metrics_names:\n",
    "        if len(results[metric]) > 0:\n",
    "            mean_metric_data += create_tag(metric, {}, compute_mean_for_metric(results[metric]))\n",
    "        \n",
    "    means_tag = create_tag(\"means\", {}, newline + mean_metric_data)\n",
    "    story_tags = []\n",
    "    #create story_tags\n",
    "    for story in result_by_story:\n",
    "        param = {\"id\" : story}\n",
    "        metric_data = newline\n",
    "        for metric in metrics_names:\n",
    "            if metric in result_by_story[story]:\n",
    "                metric_tag = create_tag(metric, [], result_by_story[story][metric])\n",
    "                metric_data += metric_tag\n",
    "        \n",
    "        story_tags.append(create_tag(\"story\", param, metric_data))    \n",
    "    \n",
    "    stories_data = newline + means_tag + \"\".join(story for story in story_tags)\n",
    "    stories_tag = create_tag(\"stories\", {}, stories_data)\n",
    "    \n",
    "    \n",
    "    #create main tag\n",
    "    run_tag = create_tag(\"run\", {}, newline + config_tag + stories_tag)\n",
    "    \n",
    "    file = open(file_name, \"a\")\n",
    "    file.write(run_tag)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PARSE\n",
    "answer_folder = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\saved\\\\\"\n",
    "answer_file_name = answer_folder + \"answer.xml\"\n",
    "gold_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\gold1.xml\"\n",
    "mapping_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\id_to_querry.xml\"\n",
    "stop_words_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\stop_words.txt\"\n",
    "file_for_saving = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\evaluation.txt\"\n",
    "exe_folder = \"C:\\\\Qt\\\\5.5\\\\mingw492_32\\\\bin\\\\\"\n",
    "exe_file = exe_folder + \"TemporalSummarization.exe\"\n",
    "config_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\TemporalSummarization\\\\start_config.xml\"\n",
    "w2v_file_name = \"C:\\\\Users\\\\MishaDEV\\\\Data\\\\news_corp_tr_last_w5_s100_c10.bin\"\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stop_words = read_stop_words(stop_words_file_name)\n",
    "\n",
    "path_to_w2v = \"C:\\\\Users\\\\MishaDEV\\\\Data\\\\news_corp_tr_last_w5_s100_c10.bin\"\n",
    "w2v_model = Word2Vec.load_word2vec_format(path_to_w2v, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PARSE QUERIES\n",
    "mapping = parse_mapping(mapping_file_name)\n",
    "queries = mapping.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "args = [exe_file]\n",
    "args.append(str(len(queries)))\n",
    "for query in queries:\n",
    "    args.append(str(query))\n",
    "    \n",
    "args_tail = [\"-a\", answer_file_name, \"-c\", config_file_name, \"-e\", w2v_file_name]\n",
    "args += args_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CALL TSS\n",
    "subprocess.call(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PARSE ANSWERS\n",
    "answers = answer_parser(answer_file_name)\n",
    "golds = answer_parser(gold_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\r",
      "6.666666666666667%\r",
      "13.333333333333334%\r",
      "20.0%\r",
      "26.666666666666668%\r",
      "33.333333333333336%\r",
      "40.0%\r",
      "46.666666666666664%\r",
      "53.333333333333336%\r",
      "60.0%\r",
      "66.66666666666667%\r",
      "73.33333333333333%\r",
      "80.0%\r",
      "86.66666666666667%\r",
      "93.33333333333333%\r"
     ]
    }
   ],
   "source": [
    "#EVALUATE\n",
    "metrics = {\"R1\" : {}, \"P1\" : {}, \"F1\" : {}, \"Psent1\" : {}, \"R2\" : {}, \"P2\" : {}, \"F2\" : {}, \"Psent2\" : {}}\n",
    "\n",
    "compute_all_metrics(metrics, answers, golds, mapping, w2v_model) \n",
    "\n",
    "save_evaluation_in_file(\"\", metrics, file_for_saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
