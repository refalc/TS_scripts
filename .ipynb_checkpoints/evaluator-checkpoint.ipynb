{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answer_folder = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\build-TemporalSummarization-Desktop_Qt_5_5_1_MinGW_32bit-Release\\\\\"\n",
    "answer_file_name = answer_folder + \"answer.xml\"\n",
    "gold_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\gold1.xml\"\n",
    "mapping_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\id_to_querry.xml\"\n",
    "stop_words_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\stop_words.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import pymorphy2\n",
    "import codecs\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def read_stop_words(file_name):\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    stop_words = file.read().lower().split()\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "stop_words = read_stop_words(stop_words_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "\n",
    "### ANSWER PARSER\n",
    "mystem = Mystem(entire_input=False)\n",
    "def GetNormalForm(text) :\n",
    "    if type(text) != type(\"str\") :\n",
    "        return []\n",
    "    morph_data = mystem.analyze(text)\n",
    "    normal_forms = []    \n",
    "    for data in morph_data :\n",
    "        if len(data) == 0 :\n",
    "            continue\n",
    "\n",
    "        if len(data[\"analysis\"]) == 0 :\n",
    "            continue\n",
    "    \n",
    "        normal_forms.append(data[\"analysis\"][0][\"lex\"])\n",
    "        \n",
    "    return normal_forms\n",
    "\n",
    "def GetNormalFormPymorphy2(text):\n",
    "    words = text.split()\n",
    "    normal_forms = []\n",
    "    for word in words:\n",
    "        morph_data = morph.parse(word)\n",
    "        if len(morph_data) > 0:\n",
    "            normal_forms.append(morph_data[0].normal_form)\n",
    "            \n",
    "    return normal_forms\n",
    "            \n",
    "def clean_text_data(text):\n",
    "    punct_set = string.punctuation\n",
    "    punct_set += '»'\n",
    "    punct_set += '«'\n",
    "    punct_set += '“'\n",
    "    punct_set += '„'\n",
    "    translator = str.maketrans('', '', punct_set)\n",
    "    \n",
    "    text = text.translate(translator).lower()\n",
    "    splited_text = text.split()\n",
    "    cleared_list = []\n",
    "    for word in splited_text:\n",
    "        if word not in stop_words:\n",
    "            cleared_list.append(word)\n",
    "    \n",
    "    text = ' '.join(word for word in cleared_list)\n",
    "    \n",
    "    return text\n",
    "def answer_parser(file_name):\n",
    "    answer_data = dict()\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    text = file.read()\n",
    "    \n",
    "    #del newline\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    # del metadata\n",
    "    text = re.sub(r\"<metadata(.*?)>\", \"\", text)\n",
    "    \n",
    "    #del querry data\n",
    "    text = re.sub(r\"<querries>(.*?)</querries>\", \"\", text)\n",
    "    stories = re.findall(r\"(<story.*?)</story>\", text)\n",
    "    for story in stories:\n",
    "        story_id = \"\"\n",
    "        if re.search(r\"init_doc_id=(\\d*)\", story) :\n",
    "            story_id = re.search(r\"init_doc_id=(\\d*)\", story).group(1)\n",
    "        else:\n",
    "            story_id = re.search(r\"story id=(\\d*)\", story).group(1)\n",
    "            \n",
    "        sentences = re.findall(r\"(<sentence.*?)</sentence>\", story)\n",
    "        answer_data[story_id] = []\n",
    "        for sentence in sentences:\n",
    "            sent_data = re.search(r\"<sentence id=(\\d*)(.*?)>(.*)\", sentence)\n",
    "            \n",
    "            sent_text = sent_data.group(3)\n",
    "            sent_text = clean_text_data(sent_text)\n",
    "            answer_data[story_id].append(sent_text)\n",
    "            \n",
    "    return answer_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_mapping(file_name):\n",
    "    mapping = dict()\n",
    "    file = codecs.open(file_name, \"r\", \"utf_8_sig\")\n",
    "    text = file.read()\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    \n",
    "    pairs = re.findall(r\"<pair>(.*?)</pair>\", text)\n",
    "    for pair in pairs:\n",
    "        story_id = re.search(r\"<id>(\\d*)</id>\", pair).group(1)\n",
    "        queries = re.findall(r\"<doc_id>(.*?)</doc_id>\", pair)\n",
    "        for query in queries:\n",
    "            mapping[query] = story_id\n",
    "            \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = parse_mapping(mapping_file_name)\n",
    "answers = answer_parser(answer_file_name)\n",
    "golds = answer_parser(gold_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ngramms(input_strs, N):\n",
    "    ngramms = []\n",
    "    for i in range(0, len(input_strs) - N + 1):\n",
    "        ngramm = input_strs[i]\n",
    "        for j in range(i + 1, i + N):\n",
    "            ngramm += \" \" + input_strs[j]\n",
    "        ngramms.append(ngramm)\n",
    "\n",
    "    return set(ngramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "path_to_w2v = \"C:\\\\Users\\\\MishaDEV\\\\Data\\\\news_corp_tr_last_w5_s100_c10.bin\"\n",
    "w2v_model = Word2Vec.load_word2vec_format(path_to_w2v, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "\n",
    "def compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent):\n",
    "    all_retr = set()\n",
    "    all_rel = set()\n",
    "    for sentence_data in retr_ngrams_by_sent:\n",
    "        all_retr.update(sentence_data)\n",
    "        \n",
    "    for sentence_data in rel_ngrams_by_sent:\n",
    "        all_rel.update(sentence_data)\n",
    "        \n",
    "    RPF = [0, 0, 0]\n",
    "    if len(all_retr) > 0 and len(all_rel) > 0:\n",
    "        P = len(all_rel.intersection(all_retr)) / len(all_retr)\n",
    "        R = len(all_rel.intersection(all_retr)) / len(all_rel)\n",
    "        F = 2 * P * R / (P + R)\n",
    "        RPF[0] = R\n",
    "        RPF[1] = P\n",
    "        RPF[2] = F\n",
    "        \n",
    "    return RPF\n",
    "        \n",
    "def create_embeding_for_ngramm(ngramm, model):\n",
    "    vec = numpy.zeros(100)\n",
    "    splited = ngramm.split()\n",
    "    \n",
    "    count = 0\n",
    "    for word in splited:\n",
    "        n_form = GetNormalFormPymorphy2(word)[0]\n",
    "        if n_form in model.wv.vocab:\n",
    "            vec += model[n_form]\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def create_embeddings(ngramms, model):\n",
    "    embeddings = []\n",
    "    for ngramm in ngramms:\n",
    "        embeddings.append(create_embeding_for_ngramm(ngramm, model))\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "def sentences_intersect_by_embedd(sent_retr, sent_rel):\n",
    "    intersect_count = 0\n",
    "    red_line = 0.5\n",
    "    for ngramm_emb_rel in sent_rel:\n",
    "        for ngramm_emb_retr in sent_retr:\n",
    "            if cosine_similarity([ngramm_emb_rel], [ngramm_emb_retr])[0][0] > red_line:\n",
    "                intersect_count += 1\n",
    "                break\n",
    "    \n",
    "    return intersect_count\n",
    "            \n",
    "def compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model):\n",
    "    red_line = 0.7\n",
    "    success_hits = 0\n",
    "    \n",
    "    vecs_for_set_sentence_rel = []\n",
    "    for rel_sentence_ngramms in rel_ngrams_by_sent:\n",
    "        vecs_for_set_sentence_rel.append(create_embeddings(rel_sentence_ngramms, model))\n",
    "       \n",
    "    vecs_for_set_sentence_retr = []\n",
    "    for retr_sentence_ngramms in retr_ngrams_by_sent:\n",
    "        vecs_for_set_sentence_retr.append(create_embeddings(retr_sentence_ngramms, model))\n",
    "        \n",
    "\n",
    "    for rel_sentence_embedds in vecs_for_set_sentence_rel:\n",
    "        for retr_sentence_embedds in vecs_for_set_sentence_retr:\n",
    "            intersect_count = sentences_intersect_by_embedd(retr_sentence_embedds, rel_sentence_embedds)\n",
    "            cur_score = intersect_count / len(rel_sentence_embedds) \n",
    "            if cur_score > red_line:\n",
    "                success_hits += 1\n",
    "                break\n",
    "                \n",
    "    return success_hits / len(rel_ngrams_by_sent)\n",
    "    \n",
    "    \n",
    "metrics = {\"R1\" : {}, \"P1\" : {}, \"F1\" : {}, \"Psent1\" : {}, \"R2\" : {}, \"P2\" : {}, \"F2\" : {}, \"Psent2\" : {}}\n",
    "\n",
    "def compute_all_metrics(metrics, answers, golds, mapping, model):\n",
    "    count = 0\n",
    "    all_size = len(answers)\n",
    "    for story in answers:\n",
    "        retrieved_sentences = answers[story]\n",
    "        relevant_sentences = golds[mapping[story]]\n",
    "        retr_ngrams_by_sent = []\n",
    "        rel_ngrams_by_sent = []\n",
    "        for sentence in retrieved_sentences:\n",
    "            retr_ngrams_sent = create_ngramms(sentence.split(), 1)\n",
    "            retr_ngrams_by_sent.append(set(retr_ngrams_sent))\n",
    "        for sentence in relevant_sentences:\n",
    "            rel_ngrams_sent = create_ngramms(sentence.split(), 1)\n",
    "            rel_ngrams_by_sent.append(set(rel_ngrams_sent))\n",
    "\n",
    "        RPF = compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent)\n",
    "        metrics[\"R1\"][story] = RPF[0]\n",
    "        metrics[\"P1\"][story] = RPF[1]\n",
    "        metrics[\"F1\"][story] = RPF[2]\n",
    "        metrics[\"Psent1\"][story] = compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model)\n",
    "        \n",
    "        retr_ngrams_by_sent = []\n",
    "        rel_ngrams_by_sent = []\n",
    "        for sentence in retrieved_sentences:\n",
    "            retr_ngrams_sent = create_ngramms(sentence.split(), 2)\n",
    "            retr_ngrams_by_sent.append(set(retr_ngrams_sent))\n",
    "        for sentence in relevant_sentences:\n",
    "            rel_ngrams_sent = create_ngramms(sentence.split(), 2)\n",
    "            rel_ngrams_by_sent.append(set(rel_ngrams_sent))\n",
    "    \n",
    "        RPF = compute_RPF(retr_ngrams_by_sent, rel_ngrams_by_sent)\n",
    "        metrics[\"R2\"][story] = RPF[0]\n",
    "        metrics[\"P2\"][story] = RPF[1]\n",
    "        metrics[\"F2\"][story] = RPF[2]\n",
    "        #metrics[\"Psent2\"][story] = compute_Psent(retr_ngrams_by_sent, rel_ngrams_by_sent, model)\n",
    "        if count % 1 == 0:\n",
    "            print(str(100 * count / all_size) + \"%\", end=\"\\r\")\n",
    "        count += 1\n",
    "    \n",
    "    return metrics\n",
    "        \n",
    "        \n",
    "def compute_mean_for_metric(metric):\n",
    "    mean = 0\n",
    "    for story in metric:\n",
    "         mean += metric[story]\n",
    "    if len(metric) > 0:\n",
    "        mean /= len(metric)\n",
    "    return mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1 : 0.35575395973630547\n",
      "P1 : 0.2047346988232779\n",
      "F1 : 0.2559178145041231\n",
      "Psent1 : 0.3162706492853552\n",
      "R2 : 0.19743499603293263\n",
      "P2 : 0.09119156268027792\n",
      "F2 : 0.1227691541962909\n",
      "Psent2 : 0\n"
     ]
    }
   ],
   "source": [
    "compute_all_metrics(metrics, answers, golds, mapping, w2v_model)  \n",
    "\n",
    "metrics_names = [\"R1\", \"P1\", \"F1\", \"Psent1\", \"R2\", \"P2\", \"F2\", \"Psent2\"]\n",
    "for metric in metrics_names:\n",
    "    mean = compute_mean_for_metric(metrics[metric])\n",
    "    print(metric + \" : \" + str(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PARSE\n",
    "answer_folder = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\TemporalSummarization\\\\build-TemporalSummarization-Desktop_Qt_5_5_1_MinGW_32bit-Release\\\\\"\n",
    "answer_file_name = answer_folder + \"answer.xml\"\n",
    "gold_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\gold1.xml\"\n",
    "mapping_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\id_to_querry.xml\"\n",
    "stop_words_file_name = \"C:\\\\!DEV\\\\C++\\\\Diplom\\\\GoldSummary\\\\stop_words.txt\"\n",
    "\n",
    "mapping = parse_mapping(mapping_file_name)\n",
    "answers = answer_parser(answer_file_name)\n",
    "golds = answer_parser(gold_file_name)\n",
    "\n",
    "path_to_w2v = \"C:\\\\Users\\\\MishaDEV\\\\Data\\\\news_corp_tr_last_w5_s100_c10.bin\"\n",
    "w2v_model = Word2Vec.load_word2vec_format(path_to_w2v, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COMPUTE\n",
    "metrics = {\"R1\" : {}, \"P1\" : {}, \"F1\" : {}, \"Psent1\" : {}, \"R2\" : {}, \"P2\" : {}, \"F2\" : {}, \"Psent2\" : {}}\n",
    "compute_all_metrics(metrics, answers, golds, mapping, w2v_model)  \n",
    "\n",
    "metrics_names = [\"R1\", \"P1\", \"F1\", \"Psent1\", \"R2\", \"P2\", \"F2\", \"Psent2\"]\n",
    "for metric in metrics_names:\n",
    "    mean = compute_mean_for_metric(metrics[metric])\n",
    "    print(metric + \" : \" + str(mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
